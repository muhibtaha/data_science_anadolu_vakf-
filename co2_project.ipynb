{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a8227d-23de-46b1-8d99-4dbc37d4c030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CO2 Emissions Calculator Notebook\n",
    "\n",
    "## 1. Download and Load the Dataset from Kaggle\n",
    "\n",
    "\n",
    "# pandas: For working with tables (DataFrames)\n",
    "import pandas as pd\n",
    "# numpy: For numerical calculations and arrays\n",
    "import numpy as np\n",
    "# sklearn: For machine learning\n",
    "from sklearn.model_selection import train_test_split # To split data for training and testing\n",
    "from sklearn.tree import DecisionTreeRegressor      # Decision Tree\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error # To measure how good the model is\n",
    "# matplotlib and seaborn: For plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# kagglehub: To download datasets from Kaggle\n",
    "import kagglehub\n",
    "# os: For file and folder operations (like listing files in a folder)\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285860f0-fe54-47d9-a229-fd06dc32e66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Download and Load the Dataset from Kaggle\n",
    "print(\"\\n--- 1. Download and Load the Dataset from Kaggle ---\")\n",
    "\n",
    "dataset_path = kagglehub.dataset_download(\"anshtanwar/global-data-on-sustainable-energy\")\n",
    "print(f\"Folder where dataset files are located: {dataset_path}\")\n",
    "\n",
    "csv_file_name = None\n",
    "# All files in the downloaded folder\n",
    "for file in os.listdir(dataset_path):\n",
    "    if file.endswith('.csv'):\n",
    "        csv_file_name = file\n",
    "        break\n",
    "\n",
    "if csv_file_name is None:\n",
    "    print(f\"\\nOH NO! We couldn't find a CSV file in the '{dataset_path}' folder.\")\n",
    "    print(\"There might have been a problem with the download. Please check.\")\n",
    "    exit() # We can't continue without a CSV file, let's stop the program\n",
    "\n",
    "full_csv_path = os.path.join(dataset_path, csv_file_name)\n",
    "\n",
    "df = pd.read_csv(full_csv_path)\n",
    "print(f\"\\nGreat! We successfully read the '{full_csv_path}' file and loaded it into the 'df' table.\")\n",
    "\n",
    "print(\"\\nFirst look at our data table (first 5 rows):\")\n",
    "print(df.head()) # head() shows us the first few rows of the table\n",
    "\n",
    "print(\"\\nWhat is the size of our table? (number of rows, number of columns):\")\n",
    "print(df.shape)\n",
    "\n",
    "print(\"\\nWhat are the names of the columns in our table?:\")\n",
    "print(df.columns)\n",
    "\n",
    "print(\"\\nMore detailed information about our columns (data types, are there any missing values?):\")\n",
    "df.info() # info() tells the type of each column and how many non-null values there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053da5e1-680d-406b-860a-29d4d5edd1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: What Will We Predict? (Target Variable) and Which Information Will We Use? (Features)\n",
    "print(\"\\n--- 2. Define Our Target and the Information We Will Use ---\")\n",
    "# What we want to predict: 'Value_co2_emissions_kt_by_country'\n",
    "# i.e., CO2 emission amounts of countries\n",
    "target_column_name = 'Value_co2_emissions_kt_by_country'\n",
    "\n",
    "# does this column really exist in our table?\n",
    "if target_column_name not in df.columns:\n",
    "    print(f\"\\nOH NO! It seems there is no column named '{target_column_name}' in our table.\")\n",
    "    print(\"Are you sure you wrote the column name correctly? Or the dataset might have changed.\")\n",
    "    exit() # We can't continue if the target column is missing\n",
    "print(f\"\\nAlright! Our target is set: {target_column_name}\")\n",
    "\n",
    "# Rows where the target column (CO2 emission) has no value (is null)\n",
    "# let's remove these rows from the table,\n",
    "df_filtered = df.dropna(subset=[target_column_name])\n",
    "print(f\"\\nNumber of rows with a value (non-null) in the '{target_column_name}' column: {df_filtered.shape[0]}\")\n",
    "\n",
    "# what if no rows are left?\n",
    "if df_filtered.empty:\n",
    "    print(f\"\\nOops! After cleaning the nulls in the '{target_column_name}' column, no data remained.\")\n",
    "    print(\"This is a bit strange. You might want to check the dataset.\")\n",
    "    exit()\n",
    "\n",
    "# If we deleted too many rows, let's give a warning\n",
    "if df_filtered.shape[0] < 0.5 * df.shape[0]:\n",
    "    print(f\"CAUTION: We removed a significant portion of the data due to null values in the target column. This might affect the model's result.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbd3c98-ffad-4c1c-a1d6-8607660525c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3: Prepare the Data for Machine Learning (Data Preprocessing)\n",
    "print(\"\\n--- 3. Let's Organize Our Data a Bit for Machine Learning ---\")\n",
    "\n",
    "# Text-containing columns like 'Entity' (country name) are disabled for now\n",
    "numeric_cols = df_filtered.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "# Let's remove our target column from the numerical columns, the rest will be our \"features\".\n",
    "features_candidates = [col for col in numeric_cols if col != target_column_name]\n",
    "print(f\"\\nNumerical features (columns) we consider using to train the model: {features_candidates}\")\n",
    "\n",
    "# What if we can't find any numerical features?\n",
    "if not features_candidates:\n",
    "    print(\"\\nOH NO! We couldn't find any numerical features to use for the model.\")\n",
    "    print(\"There might be an issue with the dataset or the column types might be different.\")\n",
    "    exit()\n",
    "\n",
    "# Our features as 'X', our target as 'y'\n",
    "X = df_filtered[features_candidates].copy()\n",
    "y = df_filtered[target_column_name].copy()\n",
    "\n",
    "for col in X.columns:\n",
    "    if X[col].isnull().any(): # If there is at least one null value in this column\n",
    "        mean_val = X[col].mean() # Calculate the mean of the column\n",
    "        X[col].fillna(mean_val, inplace=True) # Fill the nulls with this mean\n",
    "        print(f\"Nulls in the '{col}' column were filled with the mean value ({mean_val:.2f}).\")\n",
    "\n",
    "print(\"\\nLet's look at the final state of our features (X) (first 5 rows):\")\n",
    "print(X.head())\n",
    "print(\"\\nTotal number of null values remaining in our features (X) (hopefully zero!):\")\n",
    "print(X.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83f667b-ec7b-41ab-bec5-a6fb557143b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 4: Split the Data for Training and Testing the Model\n",
    "print(\"\\n--- 4. Let's Split Our Data into Training and Test Sets ---\")\n",
    "\n",
    "# We will train the model with a portion of our data (80%),\n",
    "# and test how well the model has learned with the remaining portion (20%).\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nNumber of features for training (X_train): {X_train.shape}\")\n",
    "print(f\"Number of features for testing (X_test): {X_test.shape}\")\n",
    "print(f\"Number of targets for training (y_train): {y_train.shape}\")\n",
    "print(f\"Number of targets for testing (y_test): {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9816fdb8-b80e-41d6-9c96-6accde0f9b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 5: Build and Train Our Decision Tree Model\n",
    "print(\"\\n--- 5. Let's Create and Train Our Decision Tree Model ---\")\n",
    "\n",
    "# We are creating a Decision Tree Regressor model.\n",
    "# max_depth: Maximum depth of the tree. If too deep, it might overfit.\n",
    "# random_state: For reproducibility of results.\n",
    "# min_samples_split: Minimum number of samples required to split a node.\n",
    "# min_samples_leaf: Minimum number of samples required at a leaf node.\n",
    "dt_regressor = DecisionTreeRegressor(max_depth=8, random_state=42, min_samples_split=10, min_samples_leaf=5)\n",
    "\n",
    "# We are training our model with the training data. So, it learns to predict y_train by looking at X_train.\n",
    "dt_regressor.fit(X_train, y_train)\n",
    "print(\"\\nFantastic! Our Decision Tree model has been successfully trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab02fb98-f409-4c81-baca-671692f7519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 6: Measure How Successful Our Model Is\n",
    "print(\"\\n--- 6. Let's Look at Our Model's Report Card: Performance Evaluation ---\")\n",
    "\n",
    "# Predictions for X_test values using the trained model\n",
    "y_pred_dt = dt_regressor.predict(X_test)\n",
    "\n",
    "# MSE (Mean Squared Error): Average of the squares of the errors. More sensitive to large errors.\n",
    "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
    "# RMSE (Root Mean Squared Error): Square root of MSE. In the same unit as the target variable.\n",
    "rmse_dt = np.sqrt(mse_dt)\n",
    "# MAE (Mean Absolute Error): Average of the absolute values of the errors. Easier to interpret.\n",
    "mae_dt = mean_absolute_error(y_test, y_pred_dt)\n",
    "# R-squared (R2 Score): Shows how well the model explains the variation in the data. Close to 1 is very good.\n",
    "r2_dt = r2_score(y_test, y_pred_dt)\n",
    "\n",
    "print(f\"\\nPerformance of Our Decision Tree Regression Model:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_dt:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_dt:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_dt:.2f}\")\n",
    "print(f\"R-squared (R2) Score: {r2_dt:.2f} (The closer this value is to 1, the better!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9f5d57-e151-4cf0-8502-88bc7d647e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 7: Which Features Were More Important for Prediction? (Feature Importances)\n",
    "print(\"\\n--- 7. Which Information Was More Important for Our Model? (Feature Importances) ---\")\n",
    "\n",
    "importances = dt_regressor.feature_importances_\n",
    "feature_names = X_train.columns # Names of the features (columns)\n",
    "\n",
    "# Let's convert these into a more readable table\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "# Sort\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nImportance Levels of Features (From most important to least):\")\n",
    "print(feature_importance_df)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(10), palette=\"viridis\")\n",
    "plt.title('Top 10 Most Important Features (For CO2 Emission Prediction)')\n",
    "plt.xlabel('Importance Level (The higher, the more important)')\n",
    "plt.ylabel('Features (Our Information Sources)')\n",
    "plt.tight_layout()\n",
    "print(\"\\nPreparing feature importance graph...\")\n",
    "plt.show() # Display the graph on the screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a98f5c4-aeb4-4f28-a5ba-4e5c0c629a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 8: How Close Are Our Predictions to Actual Values? Let's Plot!\n",
    "print(\"\\n--- 8. How Close Are Our Predictions to Reality? Let's Plot! ---\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_dt, alpha=0.6, edgecolors='w', linewidth=0.5, label='Our Predictions')\n",
    "min_val = min(y_test.min(), y_pred_dt.min()) # To set the start and end of the axes\n",
    "max_val = max(y_test.max(), y_pred_dt.max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Match Line')\n",
    "plt.xlabel('Actual CO2 Emissions (kt)')\n",
    "plt.ylabel('Our Predicted CO2 Emissions (kt)')\n",
    "plt.title('Actual Values vs. Our Predictions')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "print(\"\\nPreparing comparison graph of actual and predicted values...\")\n",
    "plt.show()\n",
    "\n",
    "# Error = Actual Value - Predicted Value\n",
    "residuals = y_test - y_pred_dt\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Let's plot the histogram (frequency graph) of errors.\n",
    "sns.histplot(residuals, kde=True, bins=30) # kde=True also plots the density curve\n",
    "plt.xlabel('Error Amount (Actual Value - Predicted Value)')\n",
    "plt.ylabel('How Many Times This Error Occurred (Frequency)')\n",
    "plt.title('Distribution of Our Prediction Errors (Residuals)')\n",
    "plt.axvline(0, color='red', linestyle='--', lw=1.5) # Ideally, errors should be around 0\n",
    "plt.grid(True)\n",
    "print(\"\\nPreparing distribution graph of prediction errors...\")\n",
    "plt.show()\n",
    "# Ideally, we expect errors to show a normal distribution around 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6912337c-e16d-4bfd-a551-dedf96a855f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 9: Can We Improve Our Model a Bit More? (Simple Optimization)\n",
    "print(\"\\n--- 9. Could We Improve the Model by Changing Its Settings? (max_depth Optimization) ---\")\n",
    "\n",
    "# Let's see how the R2 score changes by altering the 'max_depth'\n",
    "# (maximum depth) setting of the Decision Tree.\n",
    "depths_to_try = [3, 5, 8, 10, 12, 15, 20] # Different depths we will try\n",
    "best_r2_so_far = -float('inf') # To store the best R2 score so far (let's start with a very small number)\n",
    "best_depth_found = None # We will write the best depth here\n",
    "optimization_results = [] # We will save the results here\n",
    "\n",
    "print(\"\\nR2 scores for different 'max_depth' values:\")\n",
    "for depth in depths_to_try:\n",
    "    # Let's create a new model, this time with a different 'max_depth'\n",
    "    temp_model = DecisionTreeRegressor(max_depth=depth, random_state=42, min_samples_split=10, min_samples_leaf=5)\n",
    "    temp_model.fit(X_train, y_train) # Train it\n",
    "    y_pred_temp = temp_model.predict(X_test) # Make predictions\n",
    "    r2_temp = r2_score(y_test, y_pred_temp) # Calculate the R2 score\n",
    "    optimization_results.append({'depth': depth, 'r2_score': r2_temp})\n",
    "    print(f\"R2 Score when max_depth = {depth} = {r2_temp:.4f}\")\n",
    "    # If this R2 score is better than any so far, let's save it as the best\n",
    "    if r2_temp > best_r2_so_far:\n",
    "        best_r2_so_far = r2_temp\n",
    "        best_depth_found = depth\n",
    "\n",
    "print(f\"\\nBest performing max_depth: {best_depth_found} (R2 Score at this depth: {best_r2_so_far:.4f})\")\n",
    "\n",
    "# Let's show the optimization results in a graph\n",
    "results_df_for_plot = pd.DataFrame(optimization_results)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.lineplot(x='depth', y='r2_score', data=results_df_for_plot, marker='o', linestyle='-')\n",
    "plt.title('Effect of Different Maximum Depth (max_depth) Values on R2 Score')\n",
    "plt.xlabel('Maximum Depth (max_depth)')\n",
    "plt.ylabel('R2 Score (On Test Set)')\n",
    "plt.xticks(depths_to_try) # Let's show all tried depths on the X-axis\n",
    "plt.grid(True, which=\"both\", ls=\"--\")\n",
    "print(\"\\nPreparing max_depth optimization graph...\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab58b215-c210-45df-9bb0-bec42c9fd1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 10. What We Learned and What's Next? ---\")\n",
    "print(f\"* Our model, with its best setting (max_depth={best_depth_found}), could explain approximately {best_r2_so_far*100:.1f}% of the variation in CO2 emissions. (R2 Score: {best_r2_so_far:.3f})\")\n",
    "print(\"* We saw which information (features) was more useful in predicting CO2 emissions.\")\n",
    "print(\"* Thanks to the graphs we plotted, we better understood our model's predictions and errors.\")\n",
    "print(\"\\nWhat can be done next (Ideas):\")\n",
    "print(\"  * Maybe we can include text information like 'Entity' (country name) in the model somehow (e.g., One-Hot Encoding).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
